
# Project Objective
Define & maintain common, modular web crawling infrastructure that multiple projects can depend on and share the results of.

# Timelines
This project is being built on spare time across a number of groups & people who have a shared need for this type of infrastructure. Because it's being built on spare time, things will move in fits & starts. If you're interested in using this project or taking part it in, let us know! Even if you can't contribute, knowing about additional use cases can help us document our needs

# Milestones
We make use of (and try to maintain the accuracy of) [github milestones](https://github.com/qri-io/walk/milestones), which are the best way to track current progress of the project.

** **

# Use Cases
_how interested organizations are intending to use this software_

## [EDGI](https://envirodatagov.org)
EDGI is intending to spin this software up as a _running service_ for at least two groups that work with web crawler output: **Web Monitoring** and **Web Archiving** project.

### Web Monitoring Project
- Regular (in time) and consistent (in data and quality) captures of large list of URLs (30k and growing).
    - Proactive notification of captures that *differ* from the previous capture of the same URL (we don’t have this for any other data source, so it’s not a hard requirement)
    - The outputted data structure should make it easy to understand and traverse redirects (if we can query for historic data, then we should be able to query on that information). This impacts the above point about notifying differences.
    - “Regular” means >= 2/week (Versionista currently gets us every 2 days but occasionally fails; IA currently gets us every day.)
- Regular (in time) and complete (does a reasonably good job of getting all URLs) maps of all the pages in a site and the links between them.
    - “Pages” means things you might browse directly to. It includes, for example, PDFs, but not JS files.
    - “Regular” means >= 1/month.
    - General a word corpus here would be great, but we can also build that as higher level tool.
- Data generated by this service should be public (or handed to an entity that can make it public).
- It’s a live, running service (not just code), so the actual scraping is de-duplicated across all our projects that have scraping needs.

### Web Archiving Project
- Complient the archives of existing sources like the Internet Archive by building infrastracture that can match common archiving formats & specs (eg: WARC files)
- Re-contextualize those same archives on the distributed web (eg: IPFS, Dat)
- Regular (in time) crawls of designated dataset & "grey material" URLS
- Have fewer crawlers running against these host services by using the same fetches as the web monitoring project.

## [DocNow](https://github.com/DocNow) & [Diffengine](https://github.com/DocNow/diffengine):

> In Slack @edsu and I had talked about the value of having a service like this for [docnow](https://github.com/DocNow) and [diffengine](https://github.com/DocNow/diffengine). If I understood his needs well:
- Regular (in time) captures of URLs mentioned in a curated set of RSS feeds
    - Not sure on specific definitions of “regular” for this use-case.
- Proactive notification of captures that *differ* from the previous capture of the same URL

- @mr0grog
